---
title: "Econometrics for Social Scientists"
subtitle: "Volume I: Causal Inference and Estimation"
author: "Kevin Lingfeng Li"
format:
    pdf:
        toc: true
        toc-depth: 2
        documentclass: report
        papersize: A4
        geometry:
            - width=165mm
            - height=245mm
            - top=27mm
        number-sections: true
        number-depth: 1
        top-level-division: part
        linestretch: 1.25
        include-in-header: 
            text: |
                \usepackage{fvextra}
                \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
      % Note: setting commandchars=\\\{\} here will cause an error 
    }
---

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

------------------------------------------------------------------------

### Model Summary Statistics

Aside from coefficients, the Linear Regression model also has a few summary statistics that can help us interpret the effectiveness and fit of our model.

#### Estimated Residual Standard Deviation



#### Total Sum of Squares

The total sum of squares is the total amount of sample variation in $Y$: $$
\begin{split}
TSS & = \sum(Y_i - \bar{Y})^2 \\
& = \sum (\hat{Y}_i - \bar{Y})^2 + \sum (Y_i - \hat{Y}_i)^2
\end{split}
$$

Where TSS is the total sum of squares, SSM $\sum (\hat{Y}_i - \bar{Y})^2$ is the model sum of squares, and SSE $\sum (Y_i - \hat{Y}_i)^2$ is the sum of squared errors (that we used to fit the model).

SSM (model sum of squares) represents the part of the variation of $Y$ that is explained by the model, while SSE (sum of squared errors) represents the part of the variation of $Y$ that is not explained by the model (hence, why it is called error).

#### R-Squared Statistic

R-squared is one of the key summary statistics of our model.

::: callout-tip
## Key Definition: R-Squared

R-squared $R^2$ is a measure of the percentage of variation in $Y$, that is explained by our model (with our chosen explanatory variables). The percentage of variation in $Y$ explained by our model would be:

$$
R^2 = \frac{SSM}{TSS} = \frac{\sum (\hat{Y}_i - \bar{Y})^2}{\sum(Y_i - \bar{Y})^2}
$$
:::

Since $R^2$ shows how much of the variation in $Y$ our model explains, it is often used as a metric for how good our model is - however, don't overly focus on $R^2$, it is just one metric with its benefits and drawbacks.

#### F-Tests of Nested Models

The **F-test of Nested Models** allows us to compare different regression models. We use a smaller model as our null hypothesis, and a larger model (containing the smaller model) as our alternative hypothesis. More mathematically: $$
\begin{split}
M_0 : & E[Y] = \alpha + \beta_1 X_1 + ... + \beta_g X_g \\
M_a : & E[Y] = \alpha + \beta_1 X_1 + ... + \beta_g X_g + \beta_{g+1} X_{g+1} + ... + \beta_k X_k
\end{split}
$$

Importantly, all explanatory variables in model $M_0$ must also be in $M_a$ (hence "nested").

The F-test uses the F-test statistic. This statistic compared the $R^2$ values of the two models. Let us say the $R^2$ value of $M_0$ is notated $R^2_0$, and the $R^2$ value of $M_a$ is notated as $R^2_a$. The F-test statistic essentially measures the difference $R^2_a - R^2_0$. If the difference is sufficiently large, that means the $M_a$ model has significantly more explanatory power than $M_0$.

Mathematically, the F-test statistic is as follows, with $k_a$ being the number of explanatory variables in the alternate hypothesis:

$$
F = \frac{ R^2_{\text{change}} / df_{\text{change}} }{  (1 - R^2_a ) / [n - (k_a + 1)]}
$$

The sampling distribution of the F-statistic is the F distribution with parameters $k-a - k_0$ and $n-(k_a + 1)$ degrees of freedom. We then obtain the p-value from this distribution. The p-values of the F-statistic show the following:

-   If the p-value is very small, that means $R^2_a$ is significantly larger than $R^2_0$. This is evidence against model $M_0$, and in favour of the larger model $M_a$

-   If the p-value is large, that means $R^2_a$ is not much larger than $R^2_0$. This means there is no evidence against $M_0$, and $M_a$ is not the statistically significantly better model.

F-tests of nested models can help us determine if we should include certain extra explanatory variables. If our model with more variables is statistically significant, it is an indication that we should include those extra variables.

------------------------------------------------------------------------

### Interpreting Regression Tables

In most research papers, regression results will be presented in a table:

\begin{table}[h]
\begin{center}
\begin{tabular}{l c c c c}
\hline
 & Education & Mosques per 1,000 & Prop.in Poverty & Total Budget \\
\hline
Intercept               & $11.503^{***}$ & $1.474^{***}$ & $0.405^{***}$ & $81.983^{***}$ \\
                        & $(0.196)$      & $(0.060)$     & $(0.015)$     & $(2.980)$      \\
Treatment               & $-0.069$       & $-0.062$      & $0.009$       & $-1.760$       \\
                        & $(0.241)$      & $(0.074)$     & $(0.019)$     & $(4.179)$      \\
\hline
Num. obs.               & $562$          & $565$         & $560$         & $565$          \\
R$^2$ (full model)      & $0.000$        & $0.001$       & $0.000$       & $0.000$        \\
Adj. R$^2$ (full model) & $-0.002$       & $-0.001$      & $-0.001$      & $-0.002$       \\
\hline
\multicolumn{5}{l}{\scriptsize{$^{***}p<0.001$; $^{**}p<0.01$; $^{*}p<0.05$}}
\end{tabular}
\caption{Coefficients estimated with OLS estimator. Robust standard errors provided in Parentheses}
\label{table:coefficients}
\end{center}
\end{table}

Each column in the table represents a different model. In this table, *Education*, *Mosques per 1,000*, *Prop in Poverty*, and *Total budget* are the different models.

-   If the models have different outcome variables (the paper should make this clear), the column names are typically the outcome variable names. This table is a case of that.

-   However, often, the outcome variables are the same between all columns, and each is a slightly different model that indicates how coefficients change with different control variables.

In the table, we can see the rows *Intercept* and *Treatment**.***

-   The *Intercept* row represents the intercept $\hat{\alpha}$. The main number is the coefficient estimate, and the parentheses include the robust standard errors.

-   The *Treatment* row represents the $\hat{\beta}$ coefficient that is multiplied to the variable *Treatment*. The main number is the coefficient estimate, and the parentheses include the robust standard errors.

-   If there are more explanatory variables, they will be included in further rows, each indicated by the name of the variable in question.

-   The stars (\*\*\*) represent significance level (as indicated in the legend at the bottom of the table).

Below, we have some model summary statistics, including the number of observations, $R^2$, and adjusted $R^2$ numbers.

This table is a good example of how a typical regression table is made - so if you ever run a regression for a research paper, this is how you would report your results.

------------------------------------------------------------------------

### Implementation in R {.unnumbered}

We will first need the *fixest* package. If you have never used it before, install it as follows:

```{r, eval = FALSE}
install.packages("fixest")
```

Once you have installed it (or previously installed it), load it every time you open R as follows:

```{r, eval = FALSE}
library(fixest)
```

#### Regression

We use the *feols()* function to run a **regression**: The general syntax is as follows:

```{r, eval = FALSE}
model_name <- lm(Y ~ X1 + X2 + X3, data = mydata, se = "hetero")
summary(model_name)
```

-   Replace *model_name* with your model name, *Y* with the name of your response variable, *X1, X2...* with the name of your explanatory variable, and *mydata* with the name of your dataset.

-   Add additional explanatory variables with more $+$ signs, and you can remove down to one $X$.

-   The final argument, *se = "hetero"*, tells R to calculate heteroscedasticity-robust standard errors, which are standard in econometrics. However, if you can prove homoscedasticity, you can remove this argument for default standard errors.

We can also use the base-R *lm()* function, however, this function is unable to calculate robust standard errors. The syntax for *lm()* is the same, just without the *se = "hetero"* argument.

#### Confidence Intervals

To calculate confidence intervals, we can use the *confint()* command, and simply input the name of our model within:

```{r, eval = FALSE}
confint(model)
```

#### F-Tests of Nested Models

If we want an **F-test** between two models, we can use the *anova()* function, replacing *model1* with the name of the null hypothesis model $M_0$, and replacing *model2* with the name of the alternative hypothesis model $M_a$.

```{r, eval = FALSE}
anova(model1, model2)
```

------------------------------------------------------------------------

### Implementation in STATA {.unnumbered}

#### Regression

To run **regression** in Stata, use the *regress* function:

```         
regress Y X1 X2 X3, robust
```

-   Replace *Y* with the name of your response variable, *X1, X2...* with the name of your explanatory variable.

-   Add additional explanatory variables by simply adding more seperated by a space, and you can remove down to one explanatory variable.

-   The final argument, *robust*, tells Stata to calculate heteroscedasticity-robust standard errors, which are standard in econometrics. However, if you can prove homoscedasticity, you can remove this argument for default standard errors.

#### Confidence Intervals

To calculate **confidence intervals**, we first use the *collect* function to create a collection, *collect* prefix to store our coefficients from our regression, then *collect layout* to display the results:

```         
collect create confidence
collect _r_b _r_ci: regress Y X1 X2 X3, robust
collect layout (colname) (result)
```

------------------------------------------------------------------------

### Gauss-Markov Theorem, Exogeniety, and Homoscedasticity


## Further Topics in Linear Regression

------------------------------------------------------------------------

### Interaction Effects

Interactions, also called moderating effects, means that the effect of some $X_j$ on $Y$ is not constant, and depends on some third variable $X_k$. Essentially, $X_k$'s value changes the relationship between $X_j$ and $Y$.

For example, $Y$ could be the chance of a civil war occurring, $X_1$ is the severity of an economic crash, and $X_2$ is the development level of a country. We could quite reasonably expect that in the effect of a economic crash on a chance of civil war would be significantly higher in developing nations rather than developed.

Or in other words, the chance that a civil war occurs due to a economic crash is higher in countries like Venezuela, North Korea and Eritrea, compared to the relationship in Norway, Switzerland, and Denmark. Essentially, $X_1$'s effect on $Y$ is affected by the value of $X_2$.

::: callout-tip
## Key Definition: Interaction Effect Regression

Interaction effects are represented by two variables being multiplied together in a regression equation. In the model below, $X_1$ and $X_2$ are interacting with each other:

$$
\mathbb{E}[Y|\overrightarrow{X}] = \alpha + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \text{other explanatory variables}
$$
:::

Note: you can see in the above equation, $X_1$ and $X_2$ both are interacted, as well as have their own separate coefficients. We should always include both variables independently along with the interaction effect.

We can mathematically show that the effect of $X_1$ on $Y$ is not constant - and varies due to the value of $X_2$: $$
\begin{split}
\hat{Y} & = \hat{\alpha} + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \hat{\beta}_3 X_1 X_2 \\
\frac{\partial \hat{Y}}{\partial X_1} & = 0 + \hat{\beta}_1 + 0 + \hat{\beta}_3X_2 \\
\frac{\partial \hat{Y}}{\partial X_1} & = \hat{\beta}_1 + \hat{\beta}_3X_2
\end{split}
$$

As you can see, the relationship between $X_1$ and $Y$ here depends on the value of $X_2$. In more intuitive words, given a one unit increase in $X_1$, there is an expected $\hat{\beta}_1 + \hat{\beta_3}X_2$ increase in $Y$.

We can also find the effect of $X_2$ on $Y$: $$
\begin{split}
\hat{Y} & = \hat{\alpha} + \hat{\beta}_1 X_1 + \hat{\beta}_2 X_2 + \hat{\beta}_3 X_1 X_2 \\
\frac{\partial \hat{Y}}{\partial X_2} & = \hat{\beta}_2 + \hat{\beta}_3X_1
\end{split}
$$

With these equations, we can interpret the coefficients of our model more generally.

::: callout-note
## Interpretation of Interaction Effects

The coefficients in a regression with interaction effects are interpreted as:

-   $\hat{\beta}_1$ is the relationship between $X_1$ and $Y$, given $X_2 = 0$.

-   $\hat{\beta}_2$ is the relationship between $X_2$ and $Y$, given $X_1 = 0$.

-   $\hat{\beta}_3$ represents two things. For every one unit increase of $X_2$, the magnitude of the relationship between $X_1$ and $Y$ changes by $\hat{\beta}_3$. Similarly, for every one unit increase of $X_1$, the magnitude of the relationship between $X_2$ and $Y$ changes by $\hat{\beta}_3$.

-   $\alpha$ is still the expected value of $Y$ when all explanatory variables equal 0.

 

The coefficient $\beta_3$'s significance level tells us if there is a statistically significant interaction.

-   If $\beta_3$ is not statistically significant, we can often remove the interaction term.

-   However, if $\beta_3$ is statistically significant, that means we have found two terms that interact.
:::

------------------------------------------------------------------------

### Panel Data and Fixed Effects

#### Hierarchical and Panel Data

Hierarchical data is data that comes in different "clusters" or "levels". For example, if we have data on individuals from multiple different countries, that means our individual observations are clustered at the country-level.

Hierarchical data can also be clustered over time. For example, we might have GDP data for all countries in the world from 1960-2024. Each year (ex. 2024) will have GDP data for all countries, thus, the data is clustered by year. Data clustered over years is often referred to as **panel data** or **longitudinal** data.

Hiearchical data can be clustered over country and year at the same time. The previous example of GDP data can be clustered by year (ex. 2023, 2022, etc.) and clustered by country (ex. USA, UK, etc.).

Why do we care about clusters? Well - this is because one cluster might be very different than another cluster. For example, if we were explaining individual voting turnout between countries, different electoral and cultural factors in each country might explain some of the differences. Another example is the 2008 financial crisis, which may mean 2008 values will be different from 2015 because of circumstances surrounding each particular year.

These differences between clusters affect our regression results. For example, if we want to explain the outcome variable individual voter turnout with the explanatory variable individual education level, some of the effect of different countries and years may be captured in our regression. That means our regression is not accurately measuring the size of effects.

Thus, we need some way to control for these clusters in our data to isolate the effect of our treatment $D$ and accurately assess the causal impact.

#### Fixed Effects

Fixed Effects are a way to control for the issue of differences between clusters.

Let us assume that we have $m$ number of clusters in our data. Thus, we have a specific cluster $i \in \{1, ..., m\}$. Each cluster $i$ will have $n$ number of observations, so we will have observation $t \in \{1, ..., n\}$ within cluster $i$.

Using this framework, every observation can be defined as $Y_{it}$, which essentially means the $Y$ value of the $i$th cluster's $t$th observation. The corresponding explanatory variable values will be notated $\overrightarrow{X}_{it}$.

::: callout-tip
## Key Definition: Fixed Effects Model

A fixed effects model will take the following form:

$$
Y_{it} = \alpha_i + \beta_1X_{1it} + ... + \beta_kX_{kit} + \epsilon_{it}
$$

Where $\alpha_i$ is the fixed effect for cluster $i$, defined as:

$$
\alpha_i = \beta_{00} + \beta_{02}D){i2} + ... + \beta_{0m}D_{im}
$$

Where $D_{i2} ,..., D_{im}$ are dummy variables for the clusters $2, ..., m$. Cluster 1 is the reference category (like a categorical explanatory variable). $\beta_{00}$ is the average $Y$ of the reference cluster category (cluster 1), when $\overrightarrow{X} = 0$. $\beta_{0j}$ is the difference between the average $Y$ of cluster $j$, and the reference category (cluster 1), when all $\overrightarrow{X} = 0$.
:::

Or in other words, including fixed effects for clusters $i$ means using the clusters as an additional categorical variable in our regression. We can demonstrating this by writing out $\alpha_i$ in our above linear model to get:

$$
Y_{it} = \beta_{00} + \beta_1X_{1it} + ... + \beta_kX_{kit} + \beta_{02}D_{i2} + ... + \beta_{0m}D_{im} + \epsilon_{it}
$$

The fixed effect $\alpha_i$ captures the predictors of $Y$ that are shared by all observations within their cluster $i$. For example, if our fixed effects were by countries, $\alpha_i$ would capture all the predictors of $Y$ that are shared by all observations from that same country. To interpret our coefficients $\beta_j$, we would do the same as we previously would, but adding the line - controlling for levels of $Y$ we would expect for that cluster in general.

#### Two-Way Fixed Effects

Often in Political Economics, we will have 2-way clustered data by both country and year. For example, if you have data on GDP and Democracy level from all countries between 2006-2024, you will have two types of clusters - clusters by country, and clusters by year.

::: callout-tip
## Key Definition: Two-Way Fixed Effects

We can combine these two for two-way fixed effects of both country and year. Two-way fixed effects takes the following form:

$$
Y_{it} = \alpha_i + \gamma_t + \beta_1X_{1it} + ... + \beta_k X_{kit} + \epsilon_{it}
$$

$\alpha_i$ represents country fixed effects, exactly as we described in the previous section.

$\gamma_t$ represents year fixed effects. Why does it have the subscript $t$? Well, in panel data, for each country, you will have many different years of data (ex. USA will have data between 2006-2024). Thus, within cluster $i$ of the country, each observation $t$ is a different year. Thus, $t$ is the year of the data.
:::

To interpret our coefficients $\beta_j$, we would do the same as we previously would, but adding the line - controlling for levels of $Y$ we would expect for that country in that year in general.

This allows us to account for differences in countries and differences in years, and is very very common in Political Economics. You will also sometimes see different variations of this, including State-Year, Country-Decade, District-5Years, or any Geographic-Time clustering.

------------------------------------------------------------------------

### Polynomial Transformations

Sometimes, a linear (straight-line) best-fit line is a poor description of a relationship. We can model more flexible relationships that are not straight lines, by including a transformation of the variable $X$ that we are interested in.

#### Quadratic Transformations

::: callout-tip
## Key Definition: Quadratic Transformation

Quadratic transformations of $X_j$ take the following form:

$$
\mathbb{E}[Y_i |\overrightarrow{X}] = \alpha + \beta_1 X_{ji} + \beta_2 X^2_{ji} + \text{other explanatory variables}
$$
:::

If you recall from high-school algebra, an equation that takes the form of $y = ax^2 + bx + c$ creates a *parabola*.

-   A true parabola has a domain of $(-∞, ∞)$. However, our model often does not need to do this. The best-fit parabola is only used for the range of plausible $X$ values, given the nature of our explanatory variable.

-   For example, if $X$ was age, a negative number would make no sense. Because the parabola's domain often exceeds our plausible range of $X$ values, the vertex of the parabola (where it changes directions) may not be in our data.

We always include lower degree terms in our model. For example, in this quadratic (power 2) model, we also include the $X$ term without the square. To fit a model like this, we simply do the same process of minimising the sum of squared errors. How do we interpret the coefficients $\beta_1$ and $\beta_2$?

::: callout-note
## Interpretation of Quadratic Transformations

$\beta_1$'s value is no longer directly interpretable. This is because we cannot "hold all other coefficients constant", since $\beta_2$ also contains the same $X$ variable.

 

$\beta_2$'s value also cannot be directly interpreted. [If the coefficient of $\beta_2$ is statistically significant, we can conclude that there is a non-linear relationship between $X$ and $Y$]{.underline}. If $\beta_2$ is negative, the best-fit parabola will open downwards, and if $\beta_2$ is positive, the best-fit parabola will open upwards.
:::

If we want to interpret the magnitude of the model, we are best off using predicted values of $Y$ (obtained using the model equation above).

There is one more thing we can interpret with the quadratic transformation: the **vertex** of the best-fit parabola. The vertex, if we remember our algebra, is either the maximum or minimum point of a parabola.

If we remember from calculus and optimisation, we can find the maximum and minimums through setting the derivative equal to 0. For the quadratic model, this is as follows - we first find the derivative, then set the derivative equal to 0: $$
\begin{split}
& \hat{Y} = \hat{\alpha} + \hat{\beta}_1X + \hat{\beta}_2 X^2 \\
& \frac{d \hat{Y}}{dX} = 0 + \hat{\beta}_1 + 2\hat{\beta}_2X \\
& 0 = \hat{\beta}_1 + 2 \hat{\beta}_2X \\
& - \hat{\beta}_1 = 2 \hat{\beta}_2 X \\
& X = \frac{-\hat{\beta}_1}{2 \hat{\beta}_2}
\end{split}
$$

This point is useful, as it is either the maximum or minimum of our best-fit parabola. This means that at the $X$ value we calculate from this equation, we will either see the highest or lowest expected $Y$ value.

#### General Polynomial Models

While quadratic models are the most common polynomial transformation, we do not have to stop there. We can continue to add further polynomials (although anything beyond cubic is exceedingly rare):

-   Cubic: $E[Y] = \alpha + \beta_1 X + \beta_2 X^2 + \beta_3 X^3$

-   Quartic: $E[Y] = \alpha + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \beta_4 x^4$

Each higher order coefficient, if statistically significant, indicates that the relationship between $X$ and $Y$, is not of the previous highest power.

-   For example, if the cubic term $\beta_3$ is statistically significant, we can reject a quadratic relationship between $X$ and $Y$

Remember to always include the lower power monomials within our polynomial model. For example, if you have a quartic transformation, you must also have the linear, quadratic, and cubic terms.

### Logarithmic Transformations

Logarithmic transformations are another form of non-linear transformations. These are commonly used for heavily skewed variables, such as when the explanatory variable is income, wealth, and so on.

In situations with heavily skewed variables, we often replace $X$ in our models with $\log(X)$. Note that in statistics, when we refer to logarithms, we are referring to natural logarithms, such that $\log(X) = \ln(X)$.

::: callout-tip
## Key Definition: Logarithmic Transformation

The logarithmic transformation of explanatory variable $X_j$ takes the following form:

$$
\mathbb{E}[Y_i | \overrightarrow{X}] = \alpha + \beta_j \log(X_j) + \text{other explanatory variables}
$$
:::

**Interpretation** of the $\beta$ coefficient can be a little bit trickier for logarithmic transformations.

We could interpret it in the same way we interpret linear regressions: given a one unit increase in the log of $X$, there is an expected $\beta$ change in $Y$.

However, this issue is that this does not really say much - I mean, who knows what a *one unit increase in the log of* $X$ even means?

With some properties of logarithms, we can actually create a more useful interpretation. Based on logarithm rules, we know the following to be true: $$
\begin{split}
\log(X) + A & = \log(X) + \log(e^A) \\
& = \log(e^A \times X)
\end{split}
$$

Now, let us plug this into our original regression model: $$
\begin{split}
E[Y|X] & = \alpha + \beta\log(X) \\
E[Y|e^A \times X] & = \alpha + \beta \log(e^A \times X) \\
& = \alpha + \beta[\log(X) + A] \\
& = \alpha + \beta A + \beta \log(X)
\end{split}
$$

Now find the difference between $E[Y|e^A \times X]$ and $E[Y|X]$: $$
\begin{split}
E[Y|e^A \times X] - E[Y|X] & = [\alpha + \beta A + \beta\log(X)] - [\alpha + \beta\log(X)] \\
E[Y|e^A \times X] - E[Y|X] & = \beta A
\end{split}
$$

Thus, we can seen that when we multiply $X$ by $e^A$, we get an expected $\beta A$ change in $Y$. We can make this interpretation more useful by purposely choosing some value $A$ that makes $e^A$ make more sense. For example, if $A = 0.095$, then $e^A = 1.1$, and multiplying by 1.1 is a 10% increase.

::: callout-note
## Interpreation: Logarithmic Transformation

When $X_j$ increases by 10%, there is a expected $0.095\beta_j$ unit change in $Y$
:::

### Implementation in R {.unnumbered}

For R, we will need the package *fixest.*

```{r, eval = FALSE}
library(fixest)
```

#### Categorical Explanatory Variables

R automatically treats character/string and boolean/logical data types as categorical variables, so we can just include them in the regression like any other variable.

For variables that are numeric (but need to be represented as categorical variables), we use the as.factor() function in the linear regression. For example, below, X2 is being coerced into a factor variable:

```{r, eval = FALSE}
model <- feols(Y ~ X1 + as.factor(X2) + X3, data = mydata, se = "hetero")
summary(model)
```

We can also do this before even entering the linear regression by modifying the variable.

```{r, eval = FALSE}
mydata$X2 <- as.factor(mydata$X2)
```

If we want to change the reference category of the categorical variable, we can use the *relevel()* function (for numeric or logical factor variables, there is no need for the quotation marks).

```{r, eval = FALSE}
mydata$X2 <- as.factor(mydata$X2)
mydata$X2 <- relevel(mydata$X2, ref = "category name")
```

#### Interaction Effect

To do an interaction effect, we simply use a asterisk between the two variables we want to interact:

```{r, eval = FALSE}
model <- feols(Y ~ X1*X2 + X3, data = mydata, se = "hetero")
summary(model)
```

#### Fixed Effects

One way fixed effects: The syntax is very similar to standard linear regression, we just add fixed effects after a bar "\|". In the example below, *fix1* is the variable we are using as fixed effects:

```{r, eval = FALSE}
model <- feols(Y ~ X1 + X2 | fix1, data = mydata)
summary(model)
```

Two way fixed effects: just add another fixed effect variable after *fix1* with a "+" sign:

```{r, eval = FALSE}
model <- feols(Y ~ X1 + X2 | fix1 + fix2, data = mydata)
summary(model)
```

Important note: Often, the variable *year* is encoded as numeric, but we want it to be a categorical variable for fixed effects, so use the *as.factor()* function to coerce the variable *year*.

Also, you can do this in base-r with the *lm()* function as shown throughout the regression examples, just by including the cluster variable as a categorical explanatory variable, however, the output is not as nice.

#### Polynomial Transformations

To do a polynomial regression, we use the *I()* function within our regression model, with the first argument being the variable, and 2nd argument being the polynomial degree. For example, let us make a model with X1 with a cubic polynomial:

```{r, eval = FALSE}
model <- feols(Y ~ I(X1, 3) + X2 + X3, data = mydata, se = "hetero")
summary(model)
```

#### Logarithmic Transformations

To do a logarithmic transformation, we put the variable in question in the *log()* function. Note: if you have any 0's in the variable, this may create an error.

```{r, eval = FALSE}
model <- feols(Y ~ log(X1) + X2 + X3, data = mydata, se = "hetero")
```

------------------------------------------------------------------------

### Implementation in STATA {.unnumbered}

#### Categorical Explanatory Variables

In Stata, we simply put an *i.* in front of the variable to make STATA treat it as a categorical variable:

```         
regress Y X1 i.X2 X3, robust
```

#### Interaction Effects

In Stata, to do an interaction effect, we add two hashtags \## between the two variables we want to interact:

```         
regress Y X1##X2 X3, robust
```

#### Fixed Effects

For fixed effects, just include the fixed effects vairable names after an *i.* suffix in a normal regression. For example, a 2-way fixed effect model with variables *fix1* and *fix2*:

```         
regress Y X1 X2 X3 i.fix1 i.fix2, robust
```

#### Polynomial and Logarithmic Transformation

For polynomial and logarithmic Transformation, we just create a new variable that is the log of the old one, then use the new variable in our regression:

```         
gen X1quadratic = X1*X1
gen logX1 = log(X1)
```