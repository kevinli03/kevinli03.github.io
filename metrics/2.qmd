---
title: "Econometric Methods for Political Analysis"
subtitle: "Volume II: Causal Estimation for Observational Studies"
author: "Kevin Lingfeng Li"
format:
    pdf:
        toc: true
        toc-depth: 2
        documentclass: report
        papersize: A4
        geometry:
            - width=165mm
            - height=245mm
            - top=27mm
        number-sections: true
        number-depth: 1
        top-level-division: part
        linestretch: 1.25
        include-in-header: 
            text: |
                \usepackage{fvextra}
                \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
      % Note: setting commandchars=\\\{\} here will cause an error 
    }
---

# Maximum Likelihood Estimator

## Binomial Logistic Regression

### Binary Outcome Variables

Some outcome variables $Y$ are limited in the possible values that they can take on. This can include binary outcome variables that only take values 0 or 1, or categorical variables with 3 or more possible values.

::: callout-tip
## Key Definition: Binary Outcome Variables

**Binary Outcome Variables** have two possible values/categories: $Y=1$ and $Y=0$. Examples of binary outcome variables include yes/no, true/false, vote/abstain, treatment/control, and correct/incorrect.

 

For binary response variables, we are interested in the **proportion** of the units that are in category $Y=1$ (and knowing that, we know the rest of the units are in category $Y=0$). More formally, we define $\pi = Pr(Y=1)$, and $1-\pi = Pr(Y=0)$, where $\pi \in [0,1]$.

-   If $\pi = 0$, that means no units are in category $Y=1$ (and thus all are in category $Y = 0$).

-   If $\pi = 1$ that means all units are in category $Y=1$ (and thus none are in category $Y=0$).

We can also think of the proportion $\pi$ as a **probability** of getting an observation in category $Y=1$ if we were randomly selecting from all the observations.
:::

More formally, the binary $Y$ variable takes a Bernoulli distribution, which has only has one parameter, the probability $\pi = Pr(Y=1)$. The probability $\pi$ is also the expectation (mean) of the distribution: $\mathbb{E}[Y] = \pi$. The variance of a Bernoulli distribution is $Var(Y) = \pi (1 - \pi)$. Thus, the variance is not considered a separate parameter (unlike normal regression), since it is just a function of the main parameter $\pi$.

Sometimes, we use linear regression to model binary $Y$ variables. We previously discussed the **linear probability model**, which is exactly this. However, the linear probability model has a few key issues:

1.  A linear regression model always goes towards $±∞$ on both sides as long as the slope is not 0 (since it is a line). However, probability $\pi$ must be between 0 and 1. Thus, linear regression may give absurd predictions outside of $\pi \in [0,1]$.
2.  Assumptions of heteroscedasticity are also not possible with a binary $Y$, but that is more minor than the first issue.

Thus, we need some new model, that restricts $\pi$ estimates to within 0 and 1.

### Logistic Regression Model

Let us say we have data $(Y_i, X_{1i}, ..., X_{ki})$ for $n$ observations $i = 1, ..., n$, where $Y$ is a binary response variable with values 0 and 1. The logistic regression takes the form:

$$
\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \alpha + \beta_1 X_{1i} + ... + \beta_kX_{ki}
$$

Where $\pi_i = Pr(Y_i = 1)$, $\alpha$ and $\beta_1, ..., \beta_k$ are regression coefficients that need to be estimated.

However, this form of the model is not very useful - after all, we are interested in the probability an observation $i$ is in category $Y=1$, or in other words, we are interested in $\pi_i$. So, let us isolate $\pi$ and get the model in that form.

::: callout-tip
## Key Definition: Logistic Regression Model

The logistic regression is a model of the probability of an observation $i$ being in category $Y=1$, given the values of $\overrightarrow{X}$. The model takes the form:

$$
\pi_i = \frac{e^{\alpha + \beta_1 X_{1i} + ... + \beta_k X_{ki}} }{ 1 + e^{\alpha + \beta_1 X_{1i} + ... + \beta_k X_{ki}}}
$$

Where $\pi_i = Pr(Y_i = 1)$, $\alpha$ and $\beta_1, ..., \beta_k$ are regression coefficients that need to be estimated.
:::

The parameters of the logistic model are estimated using maximum likelihood estimation, which we will cover in detail in the next chapter.

The logistic regression function results, when plotted with one $X$ variable, results in a curve as seen in the figure below:

![](figures/2/6.1.png){fig-align="center" width="70%"}

As we can see, the curve will never exceed 0 or 1 on the $Y$ level. This allows us to have reasonable interpretations of the probability $\pi_i$. As we see from above, the parameters change the shape of the logistic curve:

-   When $\alpha$ is higher, fitted probabilities shift upward, and when $\alpha$ is lower, fitted probabilities shift downward.

-   When $\beta > 0$, increasing $X$ increases $\pi$. When $\beta<0$, increasing $X$ decreases $\pi$. The larger the absolute value of $\beta$ is, the steeper the logistic curve (the stronger the relationship).

However, the actual magnitudes of the coefficients are not directly interpretable (unlike linear regression). To do this, we need to introduce another concept: Odds Ratios.

### Odds Ratios and Interpretation

#### Odds of an Event

The **odds** are a ratio of the probabilities of an event occurring and an event not occurring. In terms of $\pi$, odds are defined as:

$$
\text{Odds} = \frac{\pi}{1 - \pi}
$$

Since $\pi$ is the probability of $Y=1$ occurring, and $1-\pi$ is the probability of $Y=1$ not occurring.

You might notice this looks very similar to the first logistic regression I presented in the last section. The logistic regression is a linear model of the log-odds: $\log \left( \frac{\pi}{1 - \pi}\right)$.

**Conditional Odds** are odds of an event occurring, given a specific explanatory variable value.

-   For example, the odds of an event occurring, given some explanatory variable $X = x$, is $\pi_x/(1 - \pi_x)$.

#### Odds Ratios

Odds ratios are a way to compare the conditional odds of an event.

::: callout-tip
## Key Definition: Odds Ratio

An odds ratio is the ratio of two conditional odds, given two different explanatory variables.

 

Let us say we have the odds of something occurring when $X=1$, which we label as $\text{odds}_1$. Then, we have another odds of something occurring when $X=0$, which we label $\text{odds}_0$. The odds ratio of these two events are:

$$
OR = \frac{\text{odds}_1}{\text{odds}_0} = \frac{\pi_1 / (1-\pi_1)}{\pi_0 / (1-\pi_0)}
$$
:::

The odds ratio is thus a ratio measure of the relationship between the odds of $Y=1$ and $X$ between two different values of $X$.

More intuitively, let us say we start with the odds of $Y=1$ when $X = 0$ ( $\text{odds}_0$ ). The odds ratio is what we would multiply $\text{odds}_0$ by in order to get the odds of $Y=1$ when $X=1$ ) ($\text{odds}_1$):

$$
\text{odds} _0 \times \frac{\text{odds}_1}{\text{odds}_0} = \text{odds}_1
$$

This tells us that odds ratios are the rate of multiplicative increase when $X$ increases by 1. This allows us to provide an interpretation of logistic regression.

-   For example, if the odds ratio is 1.2, we would multiply the odds when $X=0$ by 1.2 to get the odds when $X=1$.

-   Since multiplying something by 1.2 means increasing it by 20%, that means that when we increase $X$ from 0 to 1, the odds of $Y=1$ occurring increase by 20%.

-   Similarly, if the odds ratio was 0.9, it would mean when we increase $X$ from 0 to 1, the odds of $y=1$ occurring would decrease by 10%.

#### Calculating Odds Ratios

How can we actually calculate the odds ratios? Recall the logistic regression in terms of log-odds (but this time, with only one explanatory variable for simplicity):

$$
\log \left( \frac{\pi_i}{1 - \pi_i} \right) = \alpha + \beta X_i
$$

We can rewrite $\pi_1/(1-\pi_i)$ as $\log (\text{odds})$. Now, consider the conditional logs of $X= x$ and $X = x+1$ (an increase in 1 in $X$)..$$
\begin{split}
& \log(\text{odds}_x) = \alpha + \beta (X) \\
& \log (\text{odds}_{x+1}) = \alpha + \beta(X+1)
\end{split}
$$

The change in log-odds would thus be $\log (\text{odds}_{x+1}) - \log (\text{odds}_{x})$,which through log properties, we can simplify as well..$$
\begin{split}
\log (\text{odds}_{x+1}) - \log (\text{odds}_{x}) & = \alpha + \beta(X+1) - [\alpha + \beta X] \\
& = \alpha + \beta X + \beta - \alpha - \beta X \\
& = \beta
\end{split}
$$

We also know through property of logs, that:

$$
\log (\text{odds}_{x+1}) - \log (\text{odds}_{x}) = \log \left(\frac{\text{odds}_{x+1}}{\text{odds}_x} \right) = \beta
$$

Now we have a log of an odds ratio. We can un-log and isolate the odds ratio by taking both sides to the power of $e$, which cancels out the logs to get the final calculation of the odds ratio:

$$
\begin{split}
& e^{\log \left(\frac{\text{odds}_{x+1}}{\text{odds}_x} \right)} = e^\beta \\
& \frac{\text{odds}_{x+1}}{\text{odds}_x} = OR =  e^\beta
\end{split}
$$

Or simply, after we find the coefficients of our logistic model, we can then calculate $e^\beta$ to find the odds ratio. Thus, our interpretations are as follows:

::: callout-note
## Interpretation of Odds Ratios

We know that odds ratios are a multiplicative increase. Thus, for every one unit increase in $X$, the odds of $Y=1$ are multiplied by $e^\beta$. Or more intuitively:

-   When $e^\beta > 1$, then a one unit increase in $X$ is associated with a $(e^\beta - 1) \times 100$ percent increase in the odds of being in category $Y=1$.

-   When $e^\beta < 1$, then a one unit increase in $X$ is associated with a $(1 -e^\beta) \times 100$ percent increase in the odds of being in category $Y=1$.

-   When $e^\beta = 1$, then a one unit increase in $X$ has no effect on the odds of a unit being in category $Y = 1$.
:::

### Confidence Intervals and Hypothesis Tests

### Likelihood Ratio Test of Models

### Implementation in R {.unnumbered}

### Implementation in STATA {.unnumbered}

## Maximum Likelihood Estimation

### Likelihood Functions

See lecture 9 econometrics II slides 30-31, MY452A 10-11

### Simple Maximum Likelihood Estimator

MY452A page 12-13

### Gradient Descent Estimation Approaches

MY452A pg 14-18

### Properties of the ML Estimator

MY452A Pg 19, Metrics II pg. 37- 38.

### Probit Model

## Multinomial and Ordinal Logistic Regression

## Regression for Counts


# Selection on Observables

## Selection on Observables Design

## Matching, Weighting, and Regression

## Partial Identification and Sensitivity Analysis

