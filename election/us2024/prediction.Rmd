---
title: "Presidential Prediction"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float:
      collapsed: false
    theme: lumen
date: "2024-11-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Predictions**

Disclamer: i collected all the data and ran all this models in like 2 hours. I did not put much effort into this, its not very good. Just for fun. Might add ridge regression later but lazy.

-   Also, this is a very unique presidential election - may be flawed to assume that past data can predict what will happen this time.

### Electoral Votes Predictions

**With Height Excluded (Since Perhaps Height only affects voting for male candidates?)**

Note, % var explained is not very high (around 52% in highest model), so not quite useful

-   Bagging: Harris 226.943 Electoral Votes, [Trump 311.056]{.underline}

-   Random Forest (19 Variables Bootstrap Sampled): Harris 230.164 Electoral Votes, [Trump 307.835]{.underline}

    -   (Higher % of variance explained in electoral votes)

-   Random Forest (5 Variables Bootstrap Sampled): [Harris 274.141]{.underline} Electoral Votes, Trump 263.858

    -   (Lower % of variance explained in electoral votes, but standard RF has sqrt(x variable \#)

-   Linear Regression: [Harris 288.845 Electoral Votes]{.underline}, Trump 249.155

<br />

**With Height Included**

Note, % var explained is not very high (around 52% in highest model), so not quite useful

-   Bagging: Harris 221.320 Electoral Votes, [trump 316.679]{.underline}

-   Random Forest (20 Variables Boostrap Sampled): Harris 225.648 Electoral Votes, [trump 312.351]{.underline}

    -   (Higher % of variance explained in electoral votes)

-   Random Forest (5 Variables Bootstrap Sampled): Harris 260.041 Electoral Votes, [trump 277.958]{.underline}

-   Linear model: Harris 265.784 Electoral Votes, [trump 272.215]{.underline}

<br />

### Win/Lose Probability Predictions

**With Height Excluded**

-   Bagging Model: [61.8% Trump Wins]{.underline}, 38.2% Harris Wins

-   Random Forest (5): [59% Trump Wins]{.underline}, 41% Harris Wins

-   Naive Bayes (which is typically way overconfident, but prediction is useful): [93.98% Trump Wins]{.underline}, 6.02% Harris Wins

<br />

**With Height Included**

-   Bagging Model: [64.2% Trump wins]{.underline}, 35.8% chance Harris wins

-   Random Forest: [68% Trump Wins]{.underline}, 41% Harris Wins

-   Naive Bayes (very overconfident, cuz math mechanics of the assumptions, but anywho): [99% Trump Win]{.underline} (again, take with grain of salt)

<br />

### My Personal (Gut-Based) Predictions

**My Personal Map**

![](map.png)

<br />

### The Data in My Models

I used the following variables (that I gathered in like one hour). I did not check data quality, could be terrible.

1.  Year
2.  Incumbent Party
3.  Reelection (is on of the candidates the current president)
4.  Terms current party is in office continuously
5.  Poll margin (incumbent party candidate- challenger)
6.  Real GDP Growth in election year
7.  Unemployment rate in election rate
8.  Inflation rate in election year
9.  Incumbent party president average approval rating (gallup)
10. Incumbent party president highest approval rating (gallup)
11. Incumbent party president lowest approval rating (gallup)
12. Recession occurred in the past 4 years?
13. House net change in seats (of incumbent party) during the midterm election 2 years before the election
14. House net change in seats (of incumbent party) during the election 4 years ago
15. House change in seats (of incumbent party) for both midterm and 4 year ago election combined.
16. Incumbent party faced a primary challenge? (challenge defined as winner of primary recieved less than 60% of votes. I classify harris is no-primary challenge).
17. S&P 500 Returns in election year
18. Midterm house elections from 2 years ago, incumbent party's vote share overall (entire country)
19. \% Change in Jobs from last election to this election (a full presidential term)
20. Height of the candidates (incumbent - challenger)
21. Real estate returns in election year
22. Treasury 10-year bond returns in election year.
23. Does incumbent party have majority in House?
24. Does incumbent party have majority in Senate?
25. How many workers were furloughed as a result of a government shutdown during the current administration?
26. Did any of the 15 largest tax rises in history occur during the current administration? If so, how much was the tax rise (% wise)
27. Did any of the 15 largest tax breaks in history occur during the current administration? If so, how much was the tax break (% wise)
28. The last two data points but net.

<br />

Start with the data analysis.

Load packages, clean data

```{r, message = FALSE}
library(tidyverse)
library(randomForest)
setwd("/Users/kevinli/Documents/GitHub/kevinli03.github.io/election/us2024")
data <- read_csv("data.csv")
```

clean data

```{r}
dta <- data[-1,] # get rid of first row
dta <- dta %>%
  select(-c(Incumbent, Challenger))
```

# **Electoral College**

### Models with Height

Seed

```{r}
set.seed(32435)
```

Bagging:

```{r}
bagging <- randomForest(Pct_incumb ~ .,
                        data = dta,
                        na.action = na.omit,
                        mtry = 28,
                        importance = TRUE)
bagging
```

```{r}
#prediction
data2024 <- data[1,]
data2024 <- data2024[,-c(2,3,4)]

bagging_pred <- predict(bagging, newdata = data2024)
bagging_pred * 538
```

<br />

Random Forest (5 variables boostrap Sampled):

```{r}
forest <- randomForest(Pct_incumb ~ .,
                        data = dta,
                        na.action = na.omit,
                        mtry = 5,
                        importance = TRUE)
forest
```

```{r}
# predictions
forest_pred <- predict(forest, newdata = data2024)
forest_pred * 538
```

<br />

Random Forest (20 Variables boostrap sampled)

```{r}
forest <- randomForest(Pct_incumb ~ .,
                        data = dta,
                        na.action = na.omit,
                        mtry = 20,
                        importance = TRUE)
forest_pred <- predict(forest, newdata = data2024)
forest_pred * 538
```

Importance

```{r}
varImpPlot(bagging, type = 2)
```

```{r}
varImpPlot(forest, type = 2)
```

<br />

linear model

```{r}
lm <- lm(Pct_incumb ~ ., data = dta)
lm_pred <- predict(lm, newdata = data2024)
lm_pred * 538
```

<br />

### Models without Height

```{r}
dta_noheight <- dta %>%
  select(-height)
data2024_noheight <- data2024 %>%
  select(-height)
```

<br />

bagging

```{r}
bagging1 <- randomForest(Pct_incumb ~ .,
                        data = dta_noheight,
                        na.action = na.omit,
                        mtry = 27,
                        importance = TRUE)

bagging_pred1 <- predict(bagging1, newdata = data2024_noheight)
bagging_pred1 * 538
```

<br />

Random forst (19 variables boostrap sampled)

```{r}
forest1 <- randomForest(Pct_incumb ~ .,
                        data = dta_noheight,
                        na.action = na.omit,
                        mtry = 19,
                        importance = TRUE)

forest_pred1 <- predict(forest1, newdata = data2024_noheight)
forest_pred1 * 538
```

<br />

random forest (5 variables boostrap sampled):

```{r}
forest1 <- randomForest(Pct_incumb ~ .,
                        data = dta_noheight,
                        na.action = na.omit,
                        mtry = 5,
                        importance = TRUE)

forest_pred1 <- predict(forest1, newdata = data2024_noheight)
forest_pred1 * 538
```

<br />

linear regression

```{r}
lm1 <- lm(Pct_incumb ~ ., data = dta_noheight)
lm_pred1 <- predict(lm1, newdata = data2024_noheight)
lm_pred1 * 538
```

<br />

# **Win/Lose**

### With Height

clean data

```{r, message = FALSE}
win <- read_csv("win.csv")
win2024 <- win[1,]
win2024 <- win2024[,-2]
win <- win[-1,] # get rid of first row
win$win <- as.factor(win$win)
```

<br />

bagging

```{r}
bagging_win <- randomForest(win ~ .,
                        data = win,
                        na.action = na.omit,
                        mtry = 28,
                        importance = TRUE)
bagging_win
```

```{r}
#prediction
bagging_winpred <- predict(bagging_win, newdata = win2024, type = "prob")
bagging_winpred
```

<br />

Random Forest (5 vairables boostrap sampled):

```{r}
bagging_win <- randomForest(win ~ .,
                        data = win,
                        na.action = na.omit,
                        mtry = 5,
                        importance = TRUE)
bagging_winpred <- predict(bagging_win, newdata = win2024, type = "prob")
bagging_winpred
```

<br />

naive bayes

```{r}
library(e1071)
```

```{r}
bayes <- naiveBayes(win ~ ., data = win)
bayes_winpred <- predict(bayes, newdata = win2024, type = "raw")

bayes_winpred
```

<br />

### Without Height

```{r}
win_noheight <- win %>%
  select(-height)
win2024_noheight <- win2024 %>%
  select(-height)
```

<br />

Bagging:

```{r}
bagging_win1 <- randomForest(win ~ .,
                        data = win_noheight,
                        na.action = na.omit,
                        mtry = 27,
                        importance = TRUE)
bagging_winpred1 <- predict(bagging_win1, newdata = win2024_noheight, type = "prob")
bagging_winpred1
```

<br />

Random Forest (5 Variables bootstrapped):

```{r}
bagging_win1 <- randomForest(win ~ .,
                        data = win_noheight,
                        na.action = na.omit,
                        mtry = 5,
                        importance = TRUE)
bagging_winpred1 <- predict(bagging_win1, newdata = win2024_noheight, type = "prob")
bagging_winpred1
```

<br />

Naive Bayes:

```{r}
bayes1 <- naiveBayes(win ~ ., data = win_noheight)
bayes_winpred1 <- predict(bayes1, newdata = win2024_noheight, type = "raw")

bayes_winpred1
```
