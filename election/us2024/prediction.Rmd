---
title: "Presidential Prediction"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float:
      collapsed: false
    theme: lumen
date: "2024-11-02"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **Predictions**

Disclamer: i collected all the data and ran all this models in like 2 hours. I did not put much effort into this, its not very good. Just for fun. Might add ridge regression later but lazy.

### Electoral Votes Predictions

**With Height Excluded (Since Perhaps Height only affects voting for male candidates?)**

Note, % var explained is not very high, so not quite useful

-   Bagging: Harris 226.9437 Electoral Votes, [Trump 311.0563]{.underline}

-   Random Forest (19 Variables Bootstrap Sampled): Harris 230.1648 Electoral Votes, [Trump 307.8352]{.underline}

    -   (Higher % of variance explained in electoral votes)

-   Random Forest (5 Variables Bootstrap Sampled): [Harris 274.1415]{.underline} Electoral Votes, Trump 263.8585

    -   (Lower % of variance explained in electoral votes, but standard RF has sqrt(x variable \#)

-   Linear Regression: [Harris 288.845 Electoral Votes]{.underline}, Trump 249.155

<br />

**With Height Included**

Note, % var explained is not very high, so not quite useful

-   Bagging: Harris 221.3203 Electoral Votes, [trump 316.6797]{.underline}

-   Random Forest (20 Variables Boostrap Sampled): Harris 225.6484 Electoral Votes, [trump 312.3516]{.underline}

    -   (Higher % of variance explained in electoral votes)

-   Random Forest (5 Variables Bootstrap Sampled): Harris 260.0412 Electoral Votes, [trump 277.9588]{.underline}

-   Linear model: Harris 265.78415 Electoral Votes, [trump 272.21585]{.underline}

<br />

### Win/Lose Probability Predictions

**With Height Excluded**

-   Bagging Model: [61.8% Trump Wins]{.underline}, 38.2% Harris Wins

-   Random Forest (5): [59% Trump Wins]{.underline}, 41% Harris Wins

-   Naive Bayes (which is typically way overconfident, but prediction is useful): [93.98% Trump Wins]{.underline}, 6.02% Harris Wins

<br />

**With Height Included**

-   Bagging Model: [64.2% Trump wins]{.underline}, 35.8% chance Harris wins

-   Random Forest: [68% Trump Wins]{.underline}, 41% Harris Wins

-   Naive Bayes (very overconfident, cuz math mechanics of the assumptions, but anywho): [99% Trump Win]{.underline} (again, take with grain of salt)

<br />

### My Personal (Gut-Based) Predictions

**My Personal Map**

![](map.png)

<br />

Start with the data analysis.

Load packages, clean data

```{r, message = FALSE}
library(tidyverse)
library(randomForest)
setwd("/Users/kevinli/Documents/GitHub/kevinli03.github.io/election/us2024")
data <- read_csv("data.csv")
```

clean data

```{r}
dta <- data[-1,] # get rid of first row
dta <- dta %>%
  select(-c(Incumbent, Challenger))
```

# **Electoral College**

### Models with Height

Seed

```{r}
set.seed(32435)
```

Bagging:

```{r}
bagging <- randomForest(Pct_incumb ~ .,
                        data = dta,
                        na.action = na.omit,
                        mtry = 28,
                        importance = TRUE)
bagging
```

```{r}
#prediction
data2024 <- data[1,]
data2024 <- data2024[,-c(2,3,4)]

bagging_pred <- predict(bagging, newdata = data2024)
bagging_pred * 538
```

<br />

Random Forest (5 variables boostrap Sampled):

```{r}
forest <- randomForest(Pct_incumb ~ .,
                        data = dta,
                        na.action = na.omit,
                        mtry = 5,
                        importance = TRUE)
forest
```

```{r}
# predictions
forest_pred <- predict(forest, newdata = data2024)
forest_pred * 538
```

<br />

Random Forest (20 Variables boostrap sampled)

```{r}
forest <- randomForest(Pct_incumb ~ .,
                        data = dta,
                        na.action = na.omit,
                        mtry = 20,
                        importance = TRUE)
forest_pred <- predict(forest, newdata = data2024)
forest_pred * 538
```

Importance

```{r}
varImpPlot(bagging, type = 2)
```

```{r}
varImpPlot(forest, type = 2)
```

<br />

linear model

```{r}
lm <- lm(Pct_incumb ~ ., data = dta)
lm_pred <- predict(lm, newdata = data2024)
lm_pred * 538
```

<br />

### Models without Height

```{r}
dta_noheight <- dta %>%
  select(-height)
data2024_noheight <- data2024 %>%
  select(-height)
```

<br />

bagging

```{r}
bagging1 <- randomForest(Pct_incumb ~ .,
                        data = dta_noheight,
                        na.action = na.omit,
                        mtry = 27,
                        importance = TRUE)

bagging_pred1 <- predict(bagging1, newdata = data2024_noheight)
bagging_pred1 * 538
```

<br />

Random forst (19 variables boostrap sampled)

```{r}
forest1 <- randomForest(Pct_incumb ~ .,
                        data = dta_noheight,
                        na.action = na.omit,
                        mtry = 19,
                        importance = TRUE)

forest_pred1 <- predict(forest1, newdata = data2024_noheight)
forest_pred1 * 538
```

<br />

random forest (5 variables boostrap sampled):

```{r}
forest1 <- randomForest(Pct_incumb ~ .,
                        data = dta_noheight,
                        na.action = na.omit,
                        mtry = 5,
                        importance = TRUE)

forest_pred1 <- predict(forest1, newdata = data2024_noheight)
forest_pred1 * 538
```

<br />

linear regression

```{r}
lm1 <- lm(Pct_incumb ~ ., data = dta_noheight)
lm_pred1 <- predict(lm1, newdata = data2024_noheight)
lm_pred1 * 538
```

<br />

# **Win/Lose**

clean data

```{r, message = FALSE}
win <- read_csv("win.csv")
win2024 <- win[1,]
win2024 <- win2024[,-2]
win <- win[-1,] # get rid of first row
win$win <- as.factor(win$win)
```

<br />

bagging

```{r}
bagging_win <- randomForest(win ~ .,
                        data = win,
                        na.action = na.omit,
                        mtry = 28,
                        importance = TRUE)
bagging_win
```

```{r}
#prediction
bagging_winpred <- predict(bagging_win, newdata = win2024, type = "prob")
bagging_winpred
```

<br />

Random Forest (5 vairables boostrap sampled):

```{r}
bagging_win <- randomForest(win ~ .,
                        data = win,
                        na.action = na.omit,
                        mtry = 5,
                        importance = TRUE)
bagging_winpred <- predict(bagging_win, newdata = win2024, type = "prob")
bagging_winpred
```

<br />

naive bayes

```{r}
library(e1071)
```

```{r}
bayes <- naiveBayes(win ~ ., data = win)
bayes_winpred <- predict(bayes, newdata = win2024, type = "raw")

bayes_winpred
```

<br />

### Without Height

```{r}
win_noheight <- win %>%
  select(-height)
win2024_noheight <- win2024 %>%
  select(-height)
```

<br />

Bagging:

```{r}
bagging_win1 <- randomForest(win ~ .,
                        data = win_noheight,
                        na.action = na.omit,
                        mtry = 27,
                        importance = TRUE)
bagging_winpred1 <- predict(bagging_win1, newdata = win2024_noheight, type = "prob")
bagging_winpred1
```

<br />

Random Forest (5 Variables bootstrapped):

```{r}
bagging_win1 <- randomForest(win ~ .,
                        data = win_noheight,
                        na.action = na.omit,
                        mtry = 5,
                        importance = TRUE)
bagging_winpred1 <- predict(bagging_win1, newdata = win2024_noheight, type = "prob")
bagging_winpred1
```

<br />

Naive Bayes:

```{r}
bayes1 <- naiveBayes(win ~ ., data = win_noheight)
bayes_winpred1 <- predict(bayes1, newdata = win2024_noheight, type = "raw")

bayes_winpred1
```
