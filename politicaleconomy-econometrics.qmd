---
title: "An Introduction to Political Economy: Economic Models of Politics"
subtitle: "The first book in an Introduction to Political Economy, covering the use of economic and formal mathematical modelling in studying political phenomena"
author: "Kevin Lingfeng Li"
format:
    pdf:
        toc: true
        toc-depth: 1
        documentclass: report
        papersize: A4
        number-sections: true
        number-depth: 3
        top-level-division: part
        linestretch: 1.25
---

## Preface {.unnumbered}

This book is an introduction to the field of Political Economy - the use of economic tools, such as econometrics and game theory, applied to the study of political topics.

The book starts with a fundamental background on the idea of Political Economy, as well as some necessary Probability Theory. Next, I provide an introduction to the classic econometric/statistical techniques in the field. Then, I introduce the field of formal mathematical modelling, also called game theory. Finally, I combine the econometric techniques and formal models in the study of a few common topics in Political Economy.

This book is meant to be a relatively approachable introduction to the field. However, as Political Economy depends on many economic tools, an rough understanding of Algebra, Single Variable Calculus, and simple Linear Algebra is required. You do not need to be a math wizard, or even good at solving mathematical problems - you simply need an understanding of the intuition behind some key techniques. This book comes with a companion manual - Essential Mathematics for Political Economy. It is recommended that anyone interested in Political Economy glance at the topics covered in the manual, to ensure that they have the mathematical background necessary to succeed.

I created this book as a way to revise for my exams, as well as provide a handy booklet where I could reference all the things I learned throughout my undergraduate and postgraduate degrees. I hope that this guide to Political Economy can be useful to not just me, but others also interested in the field.

For the econometrics sections of this book, I will also use the R language for some examples, as well as providing code for some other languages (Stata, Python). This is not a coding course, so I will not introduce the basics of R.

# Fundamentals and Background

## Introduction to Political Economy

Political Economy is a term that has many different uses. Historically, the term Political Economy was used to describe the field we know today as Economics. This was particularly the case prior to the mathematical turn that the field of Economics took starting in the 20th century. Famous writers such as Adam Smith, Karl Marx, and John Locke, often identified themselves as Political Economists.


Today, Political Economy still has many different meanings. There are currently three major approaches to Political Economy - who all agree that Political Economy is somewhere between Political Science and Economics - but disagree on the approaches to studying this intersection of disciplines. First, there is the more "economics" side of Political Economy, focusing on how government and power inequalities affect economics and the distribution of resources. Second, there is the field of International Political Economy, which studies how economics interacts with International Relations. Finally, there is the field that applies economic models and methods to the study of Political and Economic phenomena.


This book mainly focuses on the third approach of Political Economy. For this book, Political Economy is a field that uses tools from economics, primarily game theory and econometrics, and applies these tools to study how political institutions, actors, and choices affect political and economic outcomes.


However, what exactly does that mean? How do we use these methods?


In the Social Sciences, there are two parts to any research - the hypothesis/theory, and the empirical data that either supports or refutes the theory. Economic tools perfectly fit into this framework. Political Economists will often use game theory and formal mathematical models to make predictions about potential outcomes of political situations. Then, Political Economists will gather real-world data, and test their hypothesis with statistical/econometric methods.


This book is the second of three in the series: An Introduction to Political Economy. The first book focuses on the economic modelling and creating hypotheses part of Political Economy Research. The next two books in the series focus on the other side of Political Economy Research - testing our hypotheses with empirical data using econometric methods.


This second book of the series begins by exploring the funamentals of Econometrics, which allow us to test the theories and hypotheses derived from our formal mathematical models. Then, we discuss regression, arguably the most important tool in Econometrics. Then, we move onto Causal Inference, and how we can make causal conclusions from empirical observational data.


Let us begin!

## Concepts in Probability

Econometrics depends on many aspects of probability. This chapter will go through what I consider the "essential" probability you must know to understand the later chapters - building on the probability we covered in book 1: An Introduction to Political Economy: Economic Models of Politics. I assume knowledge on the basics of probability and sts covered in the first book. It is highly recommended to read more about probability theory, either through a math textbook, or through consulting the "Essential Mathematics for Political Economy" companion manual.

### Random Variables

Random variables are variables that represent unobserved events that have some randomness - a set of potential outcomes, with each outcome having a probability of occurring.

For example, if you flip a coin 10 times, and count the number of heads you get, you could get 5 heads, 6 heads, 4 heads, or any amount between 0 and 10. We are not sure what will happen - however, some outcomes are more likely than others, because of the probabilities associated with each outcome.

Random variables are often called distributions - because there are a distribution of outcomes, with associated probabilities for each outcome. We can actually graph this - put potential outcomes on the $x$ axis, and the probability that each outcome occurs on the $y$ axis.

For example, take this probability distribution of a die - there are 6 sides that you could land on, and each has an equal probability of occurring:

![](figures/2.1.png){fig-align="center" width="60%"}

The **probability density function** $f(y)$ takes a potential outcome of an event as an input, and outputs the respective probability.

For example, the probability density function of a dice is $f(y) = 1/6$. This is because every outcome $y$ has the same probability of occurring: $1/6$. So $f(1), f(2)... = 1/6$.

### Expectation and Variance

Expectation and Variance are two ways we can summarise the distributions of random variables.

The **expectation**, often called the expected value or mean, is a measurement of the centre of a probability distribution. The expected value is statistically, the best guess of an outcome of a random variable, given no other information except its distribution. We notate expected value of a variable $Y$ as either $E[Y]$, $\bar{Y}$, or $\mu$.

A **discrete random variable** is one that has a countable number of distinct outcomes/categories, like the outcome of rolling a dice (6 distinct outcomes). The expected value for discrete variables is calculated by multiplying each outcome value by its associated probability, then doing that for all outcomes, and summing everything together. In other words, it is a weighted average of the outcomes, with the weights being the probability of each outcome.

$$
E[Y] = y_1 \times f(y_1) + y_2 \times f(y_2)... = \sum [y_j \times f(y_j)]
$$

For a **continuous random variable**, it is a little more complicated. This is because continuous variables have an infinite number of potential outcomes. For example, if you drive to school, your driving time could be 23 minutes, or 23.12 minutes, or 23.123324 minutes... basically, an infinite amount. As a result, we have to alter the expected value formula a little:

$$
E[Y] = \int\limits_{-∞}^∞ y \times f(y)dy
$$

**Variance** $\sigma^2$ is a measure of how spread out our distribution is. Variance basically measures how far values are, on average, from the mean of the variable. Mathematically:

$$
Var(X) = \sigma^2 = \frac{1}{n} \sum (X-\mu)^2 = E[(X - \mu)^2]
$$

Where $\sigma^2$ is the variance, $n$ is the number of observations, and $\mu$ is the mean of $X$

**Standard Deviation** $\sigma$ is the square root of variance $\sigma^2$

### Normal Distribution and T Distribution

A **normal distribution** is in the shape of a bell curve. The mean $\mu$, mode, and median are all the same value at the centre, and the distribution is symmetrical on both sides. The figure below shows the typical shape of a normal distribution

![](figures/2.2.png){fig-align="center" width="70%"}

All Normal Distributions, as shown in the image above, follow the 68-95-99.7 rule:

-   Within one standard deviation $\sigma$ of the mean $\mu$, lies 68.26% of the total area under the curve

-   Within 2 standard deviations $2 \sigma$ of the mean $\mu$, lies 95.44% of the total area under the curve

-   In fact, any amount of standard deviations $\sigma$, including decimals, is related to a specific percent of total area under the curve, for all normal distributions.

This is important, because the area under the distribution curve is actually the probability. Thus, the normal distribution tells us there is a relationship between the standard deviation and the probability of an action occurring.

Any normal distribution can be described with 2 features: mean $\mu$ and variance $\sigma^2$ in the following form: $X \sim \mathcal{N}(\mu, \sigma^2)$. For example, $X \sim \mathcal{N} (30, 4)$ means a normal distribution with mean 30 and variance 4.

The T distribution is a distribution very similar to the shape and size of the normal distribution, however, generally has thicker tails and a lower peak. The key difference is that t-distributions are defined with only one parameter - degrees of freedom $DF$.

Now, you have the fundamentals of probability you need to succeed in this course. Good Luck!

## Basics of Inference and Correlation

### Samples and Population

In political science and the social sciences, we are often interested in studying large groups of people and entities. For example, we might be interested some feature regarding all people in a country, such as the average income, or average working hours, or average education level.

However, if we are dealing with large population sizes, it is often impossible to ask every single individual in the population. For example, if we wanted to study the average educational level of the UK, we would need to ask nearly 70 million people. This is completely impractical.

A **sample** is a subset of a population, which ideally, can tell us something about the population. If our sample can reflect the greater population, then we can use the sample in our study, instead of the large population.

**Sampling** is the process by which we select a sample from a larger population, and we want the sample to be representative of the population. The quality of a sample depends on two major factors:

1.  The sampling procedure which we decide to implement
2.  Luck

Let us first talk about sampling procedure. The gold standard of sampling procedure is a random sample - where individuals in the sample are selected at random from the population. This is because in a random sample, every possible individual has an equal chance of being selected, and thus, the resulting sample is likely to be reflective of the common traits of the population.

### Central Limit Theorem

Remember how we explained that the quality of a sample depends not just on the sampling procedure, but also, luck? Well, Central Limit Theorem helps account for the luck aspect.

Before we introduce the Central Limit Theorem, we need to explain a distribution of sample means.

-   Imagine that we take a random sample from a population. Then, we find the mean of the variable we are interested in the sample. That is a sample mean.

-   Then, let us take another sample from the same population, and find the mean. This will be slightly different than the first sample, since we are randomly sampling. That is another sample mean. We keep taking samples from the same population, and getting more and more sample means.

-   Now, let us plot all our sample means into a “histogram” or density plot. The $x$ axis labels the possible sample means values, and the $y$ axis is how frequently a specific sample mean occurs. We will get a distribution, just like a random variable distribution.

-   That distribution is the d**istribution of sample means** - it basically measures the frequency of different sample means that we get, given we keep drawing samples from the same population and calculating their means.

The **Central Limit Theorem** states that the distribution of sample means of a variable, will be approximately normally distributed. This is regardless of the variable’s population distribution shape.

From chapter 2, we know of the 68-95-99.7 rule of normal distributions, and how normal distributions have a systemic relationship between the standard deviation $\sigma$ and the probabilities. The figure below reminds us of the properties of normal distributions:

![](figures/2.2.png){fig-align="center" width="70%"}

Since the distribution of sample means tells us the probability of getting some sample mean, and Central Limit Theorem tells us that distribution is normally distributed, we can now tell how likely a sample mean is to occur if a sample was drawn from the population.

-   For example, if a certain sample mean is located 2 standard deviations above the mean, there is only a 2.14% chance that that sample mean would be that value or higher (see figure above)

-   For any sample mean, we calculate how many standard deviations away from the mean of the distribution of sample means. Then, we can calculate how likely that sample mean is likely to occur.

This goes back to the “luck” aspect of sampling. What if we are unlucky in sampling, and end up randomly drawing all the tall people? All the smartest people? Well, we don't have to worry, since Central Limit Theorem tells us the likelihood of drawing a certain sample.

### Covariance and Correlation

In political science, we are often interested in the relationship between two variables. For example, are oil producers more likely to be democratic? Are more educated voters more likely to turn out and vote?

The relationship between two features, also called correlation, is the extent to which they tend to occur together.

-   A positive correlation/relationship is when we are more likely to observe feature $Y$, if feature $X$ is present

-   A negative correlation/relationship is when we are less likely to observe feature $Y$, if feature $X$ is present

-   No correlation/relationship is when we see feature $X$, that does not tell us anything about the likelihood of observing $Y$

We can also visualise these graphically:

![](figures/3.1.png){fig-align="center" width="80%"}

**Covariance** is a way to measure the relationship between two variables. Covariance is the extent that $X$ and $Y$ vary together. Mathematically:

$$
Cov(X,Y) = \sigma_{XY} = \frac{1}{n} \sum (X_i - \bar{X})(Y_i - \bar{Y})
$$

Or more simply:

-   In our data, we have many different pairs of data points $(X_i, Y_i)$

-   $X_i$ is some value of $X$, and $\bar{X}$ is the mean of $X$. Same goes for $Y_i$ and $\bar{Y}$

-   Thus, $X_i - \bar{X}$ is the distance between any point $X_i$ and the mean $\bar{X}$. Same goes for $Y_i - \bar{Y}$

-   $n$ is the number of observations (data points) in our data

We can interpret the sign of the covariance: if it is positive, we have a positive relationship. if it is negative, we have a negative relationship. However, we cannot interpret the size of the covariance.

If we want to see the strength of a relationship/correlation, we have to find the **correlation coefficient**. We calculate this by taking the covariance, and dividing it by the product of the standard deviation of $X$ and the standard deviation of $Y$. Mathematically:

$$
Corr(X,Y) = r = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}
$$

The correlation coefficient is always between -1 and 1.

-   The direction is the same as the covariance - if the coefficient is positive, then we have a positive relationship, vice versa.

-   If the correlation coefficient is closer to -1 or 1, it means a strong correlation. If the correlation is closer to 0, then it is a weak correlation

### Best Linear Predictor

While the correlation coefficient tells us the strength of a correlation, it does not say anything about the magnitude of the relationship. For example, if $X$ increases by one unit, how much does $Y$ increase by? The correlation coefficient does not say.

Magnitude is quite an important concept. After all, even if two values are very highly correlated, if an increase of one unit in $X$ only leads to a miniscule increase in $Y$, this relationship might not be very important for understanding the world.

A way to estimate the magnitude of the relationship between $X$ and $Y$ is the **best linear predictor.** The best linear predictor is a best fit line for the data, that takes the form of a linear equation: $Y = \alpha + \beta X$.

In this equation, the $\beta$ term in the best fit line is the slope of the linear equation. Essentially, it tells us for every increase in one unit of $X$, how much do we expect $Y$ to increase by?

However, how do we draw a best fit line that fits the data? After all, if we can't figure out what is the best fit, we cannot get a $\beta$ value to interpret.

-   The solution is the Ordinary Least Squares estimator, a way to esimtate $\beta$, $\alpha$ ,and all other parameters of the linear line

The Best Linear Predictor is a form of Linear Regression, the primary topic we will cover in the next two chapters.


# Regression Anlaysis

## Linear Regression Model

## Linear Regression: Transformations

## Linear Regression: Assumptions and Data

## Binomial Logistic Regression

## Multinomial and Ordinal Logistic Regression

## Regression for Counts


# Causal Inference



